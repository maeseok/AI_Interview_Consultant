{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9850859,"sourceType":"datasetVersion","datasetId":6044537},{"sourceId":9850865,"sourceType":"datasetVersion","datasetId":6044543},{"sourceId":9884793,"sourceType":"datasetVersion","datasetId":6069966},{"sourceId":9889634,"sourceType":"datasetVersion","datasetId":6073510},{"sourceId":9894110,"sourceType":"datasetVersion","datasetId":6076940},{"sourceId":9896701,"sourceType":"datasetVersion","datasetId":6078939},{"sourceId":9900354,"sourceType":"datasetVersion","datasetId":6081566},{"sourceId":9900435,"sourceType":"datasetVersion","datasetId":6081629},{"sourceId":9921546,"sourceType":"datasetVersion","datasetId":6097669},{"sourceId":9923414,"sourceType":"datasetVersion","datasetId":6099052},{"sourceId":9923424,"sourceType":"datasetVersion","datasetId":6099060},{"sourceId":9923427,"sourceType":"datasetVersion","datasetId":6099063},{"sourceId":9923457,"sourceType":"datasetVersion","datasetId":6099088},{"sourceId":9923486,"sourceType":"datasetVersion","datasetId":6099109},{"sourceId":9928729,"sourceType":"datasetVersion","datasetId":6102806},{"sourceId":9928773,"sourceType":"datasetVersion","datasetId":6102841},{"sourceId":9928779,"sourceType":"datasetVersion","datasetId":6102845}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 음성 데이터 전처리","metadata":{}},{"cell_type":"code","source":"# !pip install noisereduce","metadata":{"execution":{"iopub.status.busy":"2024-11-09T09:35:01.019262Z","iopub.execute_input":"2024-11-09T09:35:01.019657Z","iopub.status.idle":"2024-11-09T09:35:14.781697Z","shell.execute_reply.started":"2024-11-09T09:35:01.019607Z","shell.execute_reply":"2024-11-09T09:35:14.780724Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import librosa\n# import soundfile as sf\n# import noisereduce as nr\n\n# def preprocess_audio(file_path, output_path):\n#     # 1. 샘플링 레이트 고정 및 오디오 로드\n#     sr = 16000\n#     y, sr = librosa.load(file_path, sr=sr, mono=True)\n\n#     # 2. 노이즈 제거 (필요시 파라미터 조정)\n#     y_denoised = nr.reduce_noise(y=y, sr=sr)\n\n#     # 3. 음량 정규화 및 클리핑 방지\n#     rms_level = librosa.feature.rms(y=y_denoised)[0]\n#     rms_mean = rms_level.mean()\n    \n#     if rms_mean < 0.05:  # 음량이 너무 작은 경우 증폭\n#         gain_factor = 0.05 / rms_mean\n#         y_denoised = y_denoised * gain_factor\n    \n#     # 4. 무음 구간 제거 (노이즈 제거 후)\n#     y_trimmed, _ = librosa.effects.trim(y_denoised, top_db=20)\n\n#     # 5. 전처리된 파일 저장 (float32로 저장)\n#     output_file = os.path.join(output_path, os.path.basename(file_path))\n#     sf.write(output_file, y_trimmed, sr, subtype='FLOAT')\n#     print(f\"Processed file saved at: {output_file}\")\n\n# # 폴더 내 모든 .wav 파일 전처리\n# def preprocess_directory(input_dir, output_dir):\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n    \n#     for file_name in os.listdir(input_dir):\n#         if file_name.endswith(\".wav\"):\n#             file_path = os.path.join(input_dir, file_name)\n#             preprocess_audio(file_path, output_dir)\n\n# # 사용 예시\n# input_directory = \"/kaggle/input/total-data/Total_train\"\n# output_directory = \"/kaggle/working/Processed_audio_1114_train\"\n# preprocess_directory(input_directory, output_directory)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T10:14:30.074275Z","iopub.execute_input":"2024-11-01T10:14:30.075181Z","iopub.status.idle":"2024-11-01T10:14:30.098221Z","shell.execute_reply.started":"2024-11-01T10:14:30.075113Z","shell.execute_reply":"2024-11-01T10:14:30.097267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import librosa\n# import soundfile as sf\n# import noisereduce as nr\n\n# def preprocess_audio(file_path, output_path):\n#     # 1. 샘플링 레이트 고정 및 오디오 로드\n#     sr = 16000\n#     y, sr = librosa.load(file_path, sr=sr, mono=True)\n\n#     # 2. 노이즈 제거 (필요시 파라미터 조정)\n#     y_denoised = nr.reduce_noise(y=y, sr=sr)\n\n#     # 3. 음량 정규화 및 클리핑 방지\n#     rms_level = librosa.feature.rms(y=y_denoised)[0]\n#     rms_mean = rms_level.mean()\n    \n#     if rms_mean < 0.05:  # 음량이 너무 작은 경우 증폭\n#         gain_factor = 0.05 / rms_mean\n#         y_denoised = y_denoised * gain_factor\n    \n#     # 4. 무음 구간 제거 (노이즈 제거 후)\n#     y_trimmed, _ = librosa.effects.trim(y_denoised, top_db=20)\n\n#     # 5. 전처리된 파일 저장 (float32로 저장)\n#     output_file = os.path.join(output_path, os.path.basename(file_path))\n#     sf.write(output_file, y_trimmed, sr, subtype='FLOAT')\n#     print(f\"Processed file saved at: {output_file}\")\n\n# # 폴더 내 모든 .wav 파일 전처리\n# def preprocess_directory(input_dir, output_dir):\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n    \n#     for file_name in os.listdir(input_dir):\n#         if file_name.endswith(\".wav\"):\n#             file_path = os.path.join(input_dir, file_name)\n#             preprocess_audio(file_path, output_dir)\n\n# # 사용 예시\n# input_directory = \"/kaggle/input/total-data/Total_test\"\n# output_directory = \"/kaggle/working/Processed_audio_1114_test\"\n# preprocess_directory(input_directory, output_directory)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T09:35:33.703735Z","iopub.execute_input":"2024-11-09T09:35:33.704132Z","iopub.status.idle":"2024-11-09T09:36:58.448324Z","shell.execute_reply.started":"2024-11-09T09:35:33.704091Z","shell.execute_reply":"2024-11-09T09:36:58.447174Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!ls ./checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 음성 감정 분석","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nimport math\nfrom tqdm.auto import tqdm\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers.optimization import AdamW, get_constant_schedule_with_warmup\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, StochasticWeightAveraging, LearningRateMonitor\nfrom transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoFeatureExtractor, HubertForSequenceClassification, AutoConfig","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:37:32.065865Z","iopub.execute_input":"2024-11-17T03:37:32.066842Z","iopub.status.idle":"2024-11-17T03:37:39.974801Z","shell.execute_reply.started":"2024-11-17T03:37:32.066786Z","shell.execute_reply":"2024-11-17T03:37:39.973791Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 정확도 계산\ndef accuracy(preds, labels):\n    return (preds == labels).float().mean()\n\n# 2. 오디오 파일 로드\ndef getAudios(df,num):\n    audios = []\n    dir = '/kaggle/input/total-data/Total_train/' if num == 1 else '/kaggle/input/total-data/Total_test/'\n    for idx, row in tqdm(df.iterrows(),total=len(df)):\n        try:\n            # 16K 샘플링 속도로 오디오 파일 로드\n            audio, _ = librosa.load(dir + row['wav_id'] + '.wav', sr=SAMPLING_RATE)\n            audios.append(audio)\n        except Exception as e:\n            # 오류 발생 시 해당 파일을 건너뛰고 계속 진행\n            print(f\"Error loading {row['wav_id']}.wav: {e}\")\n            continue\n    return audios\n\n# 3. 배치 데이터 효율적 처리 - 패딩 관련 수정?\ndef collate_fn(samples):\n    batch_labels = [sample['label'] for sample in samples]\n    batch_audio_values = [torch.tensor(sample['audio_values']) for sample in samples]\n    batch_audio_attn_masks = [torch.tensor(sample['audio_attn_mask']) for sample in samples]\n\n    batch = {\n        'label': torch.tensor(batch_labels),\n        'audio_values': pad_sequence(batch_audio_values, batch_first=True),\n        'audio_attn_mask': pad_sequence(batch_audio_attn_masks, batch_first=True),\n    }\n\n    return batch\n    #batch_labels = torch.tensor(batch_labels)\n    # 가장 긴 시퀀스를 기준으로 길이 맞춤, 빈 공간은 0으로 채움 (padding)\n    #batch_audio_values = pad_sequence(batch_audio_values, batch_first=True)\n    #batch_audio_attn_masks = pad_sequence(batch_audio_attn_masks, batch_first=True)\n\n    #batch = {\n    #    'label': batch_labels,\n    #    'audio_values': batch_audio_values,\n    #    'audio_attn_mask': batch_audio_attn_masks,\n    #}\n\n    #return batch","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:37:43.425178Z","iopub.execute_input":"2024-11-17T03:37:43.426235Z","iopub.status.idle":"2024-11-17T03:37:43.436793Z","shell.execute_reply.started":"2024-11-17T03:37:43.426192Z","shell.execute_reply":"2024-11-17T03:37:43.435660Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install audiomentations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. 데이터셋 클래스\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, model_dim, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, model_dim)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, model_dim, 2) * -(math.log(10000.0) / model_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        \"\"\"\n        x: Tensor of shape (batch_size, seq_len, model_dim)\n        \"\"\"\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n\n\nclass MyDataset(Dataset):\n    def __init__(self, audio, audio_feature_extractor, label=None, augment=True):\n        self.label = np.array(label if label is not None else [0] * len(audio)).astype(np.int64)\n        self.audio = audio\n        # 오디오 데이터를 모델이 이해할 수 있는 형태로 변환\n        self.audio_feature_extractor = audio_feature_extractor\n        self.augment = augment\n        \n        # 데이터 증강 파이프라인\n        self.augmenter = Compose([\n            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n            Shift(min_shift=-0.5, max_shift=0.5, p=0.5),  # 변경된 인자\n        ])\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        label = self.label[idx]\n        audio = self.audio[idx]\n\n        if self.augment:\n            audio = self.augmenter(samples=audio, sample_rate=SAMPLING_RATE)\n        audio_feature = self.audio_feature_extractor(raw_speech=audio, return_tensors='np', sampling_rate=SAMPLING_RATE)\n        audio_values, audio_attn_mask = audio_feature['input_values'][0], audio_feature['attention_mask'][0]\n\n        item = {\n            'label':label,\n            'audio_values':audio_values,\n            'audio_attn_mask':audio_attn_mask,\n        }\n    \n        return item\n        \nfrom sklearn.metrics import accuracy_score\n\n# 5. 음성 분류 모델 클래스\nclass MyLitModel(pl.LightningModule):\n    def __init__(self, audio_model_name, num_labels, transformer_dim=512, nhead=4, num_layers=1, n_layers=1, projector=True, classifier=True, dropout=0.2, lr_decay=1, fold_idx=0):\n        super(MyLitModel, self).__init__()\n        self.audio_model = AutoModel.from_pretrained(audio_model_name)\n        # Positional Encoding 추가\n        self.positional_encoding = PositionalEncoding(self.audio_model.config.hidden_size)\n        # Transformer 추가\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=self.audio_model.config.hidden_size,\n                nhead=nhead,\n                dim_feedforward=transformer_dim,\n                dropout=dropout,\n                activation='gelu' #relu\n            ),\n            num_layers=num_layers\n        )\n        \n        # 분류기 추가\n        self.classifier = nn.Linear(self.audio_model.config.hidden_size, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.fold_idx = fold_idx\n        self.lr_decay = lr_decay\n        \n        # 손실 및 정확도 저장을 위한 리스트\n        self.train_losses = []\n        self.val_losses = []\n        self.train_accuracies = []\n        self.val_accuracies = []\n        \n    def forward(self, audio_values, audio_attn_mask):\n        # Hubert에서 특성 추출\n        output = self.audio_model(input_values=audio_values, attention_mask=audio_attn_mask)\n        hidden_states = output.last_hidden_state  # (batch_size, seq_len, hidden_size)\n        # Transformer에 입력\n        hidden_states = self.positional_encoding(hidden_states) ##\n        hidden_states = hidden_states.permute(1, 0, 2)  # Transformer 입력 형식: (seq_len, batch_size, hidden_size)\n        transformer_out = self.transformer(hidden_states)  # (seq_len, batch_size, hidden_size)\n        transformer_out = transformer_out.permute(1, 0, 2)  # 다시 원래 형식으로 변경: (batch_size, seq_len, hidden_size)\n\n        # 마지막 타임스텝의 출력만 사용하거나 평균 풀링\n        pooled_out = transformer_out.mean(dim=1)  # (batch_size, hidden_size)\n        \n        # 분류기 통과\n        logits = self.classifier(self.dropout(pooled_out))  # (batch_size, num_labels)\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        audio_values = batch['audio_values']\n        audio_attn_mask = batch['audio_attn_mask']\n        labels = batch['label']\n\n        logits = self(audio_values, audio_attn_mask)\n        # 이진 교차 엔트로피 손실 계산\n        loss = nn.BCEWithLogitsLoss()(logits.view(-1), labels.float())\n        \n        preds = torch.round(torch.sigmoid(logits.view(-1)))\n        # train_acc 게산\n        acc = accuracy(preds, labels.float())\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        # 손실 및 정확도 저장\n        self.train_losses.append(loss.item())\n        self.train_accuracies.append(acc.item())\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        audio_values = batch['audio_values']\n        audio_attn_mask = batch['audio_attn_mask']\n        labels = batch['label']\n\n        logits = self(audio_values, audio_attn_mask)\n        # 검증 데이터 손실 계산\n        loss = nn.BCEWithLogitsLoss()(logits.view(-1), labels.float())\n\n        preds = torch.round(torch.sigmoid(logits.view(-1)))\n        # 검증 데이터 정확도 계산\n        acc = accuracy(preds, labels.float())\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n\n        # 손실 및 정확도 저장\n        self.val_losses.append(loss.item())\n        self.val_accuracies.append(acc.item())\n        \n        return loss\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        audio_values = batch['audio_values']\n        audio_attn_mask = batch['audio_attn_mask']\n        # 최종 예측값 \n        logits = self(audio_values, audio_attn_mask)\n        preds = torch.round(torch.sigmoid(logits.view(-1)))\n\n        return preds\n\n    def configure_optimizers(self):\n        lr = 1e-5\n        layer_decay = self.lr_decay\n        weight_decay = 0.01\n        # 계층별 학습률 감쇠 적용해 파라미터 그룹 생성\n        llrd_params = self._get_llrd_params(lr=lr, layer_decay=layer_decay, weight_decay=weight_decay)\n        optimizer = AdamW(llrd_params)\n        scheduler = {\n            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True),\n            'monitor': 'val_loss',  # 검증 손실 기준으로 감소\n            'interval': 'epoch',\n            'frequency': 1\n        }\n        return [optimizer], [scheduler]\n\n    # 학습 종료 후 손실 및 정확도 그래프 저장\n    def on_train_epoch_end(self):\n        avg_train_loss = self.trainer.callback_metrics['train_loss'].item()\n        avg_train_acc = self.trainer.callback_metrics['train_acc'].item()\n        self.train_losses.append(avg_train_loss)\n        self.train_accuracies.append(avg_train_acc)\n\n    def on_validation_epoch_end(self):\n        avg_val_loss = self.trainer.callback_metrics['val_loss'].item()\n        avg_val_acc = self.trainer.callback_metrics['val_acc'].item()\n        self.val_losses.append(avg_val_loss)\n        self.val_accuracies.append(avg_val_acc)\n        \n    def on_train_end(self):\n        # 각 리스트에 맞는 epochs 생성\n        train_epochs = range(1, len(self.train_losses) + 1)\n        val_epochs = range(1, len(self.val_losses) + 1)\n        \n        # 훈련 손실 그래프\n        plt.figure(figsize=(10, 4))\n        plt.plot(train_epochs, self.train_losses, label='Train Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.title('Training Loss')\n        plt.savefig(f'./fold_{self.fold_idx}_train_loss.png')\n        plt.show()\n    \n        # 검증 손실 그래프\n        plt.figure(figsize=(10, 4))\n        plt.plot(val_epochs, self.val_losses, label='Validation Loss', color='orange')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.title('Validation Loss')\n        plt.savefig(f'./fold_{self.fold_idx}_val_loss.png')\n        plt.show()\n    \n        # 훈련 정확도 그래프\n        plt.figure(figsize=(10, 4))\n        plt.plot(train_epochs, self.train_accuracies, label='Train Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.title('Training Accuracy')\n        plt.savefig(f'./fold_{self.fold_idx}_train_accuracy.png')\n        plt.show()\n    \n        # 검증 정확도 그래프\n        plt.figure(figsize=(10, 4))\n        plt.plot(val_epochs, self.val_accuracies, label='Validation Accuracy', color='orange')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.title('Validation Accuracy')\n        plt.savefig(f'./fold_{self.fold_idx}_val_accuracy.png')\n        plt.show()\n\n        \n    # 계층별 학습률 감쇠 적용\n    def _get_llrd_params(self, lr, layer_decay, weight_decay):\n        n_layers = self.audio_model.config.num_hidden_layers\n        llrd_params = []\n        for name, value in list(self.named_parameters()):\n            if ('bias' in name) or ('layer_norm' in name):\n                llrd_params.append({\"params\": value, \"lr\": lr, \"weight_decay\": 0.0})\n            elif ('emb' in name) or ('feature' in name) : \n                llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layers+1)), \"weight_decay\": weight_decay})\n            elif 'encoder.layer' in name:\n                for n_layer in range(n_layers):\n                    if f'encoder.layer.{n_layer}' in name:\n                        llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layer+1)), \"weight_decay\": weight_decay})\n            else:\n                llrd_params.append({\"params\": value, \"lr\": lr , \"weight_decay\": weight_decay})\n        return llrd_params\n    \n    # 레이어 재초기화 \n    def _do_reinit(self, n_layers=0, projector=True, classifier=True):\n        if projector:\n            self.audio_model.projector.apply(self._init_weight_and_bias)\n        if classifier:\n            self.audio_model.classifier.apply(self._init_weight_and_bias)\n        \n        for n in range(n_layers):\n            self.audio_model.hubert.encoder.layers[-(n+1)].apply(self._init_weight_and_bias)\n    \n    # 레이어 가중치 초기화\n    def _init_weight_and_bias(self, module):                        \n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.audio_model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)   ","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:37:47.368806Z","iopub.execute_input":"2024-11-17T03:37:47.369457Z","iopub.status.idle":"2024-11-17T03:37:47.416577Z","shell.execute_reply.started":"2024-11-17T03:37:47.369414Z","shell.execute_reply":"2024-11-17T03:37:47.415841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/total-csv' # csv 파일 있는 경로\nPREPROC_DIR = '/kaggle/working/preproc'# 전처리된 데이터 저장 경로\nSUBMISSION_DIR = '/kaggle/working/submission' # 제출 파일 경로\nMODEL_DIR = '/kaggle/working/model' # 모델 저장 경로\nSAMPLING_RATE = 16000\nSEED=0 # 랜덤 시드 값\nN_FOLD=10 # 교차 검증\nBATCH_SIZE=8\nNUM_LABELS = 1 # 이진 분류","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:38:03.609678Z","iopub.execute_input":"2024-11-17T03:38:03.610370Z","iopub.status.idle":"2024-11-17T03:38:03.615732Z","shell.execute_reply.started":"2024-11-17T03:38:03.610330Z","shell.execute_reply":"2024-11-17T03:38:03.614634Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_everything(SEED) # 결과 재현성 확보\naudio_model_name = 'team-lucid/hubert-base-korean'\n\n# 특성 추출기\naudio_feature_extractor = AutoFeatureExtractor.from_pretrained(audio_model_name)\n# 마스크 반환하도록 설정해 패딩된 부분 무시\naudio_feature_extractor.return_attention_mask=True","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:38:06.089404Z","iopub.execute_input":"2024-11-17T03:38:06.089796Z","iopub.status.idle":"2024-11-17T03:38:06.368088Z","shell.execute_reply.started":"2024-11-17T03:38:06.089758Z","shell.execute_reply":"2024-11-17T03:38:06.367147Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(f'{DATA_DIR}/Total_balanced_train_data.csv', encoding='CP949')\ntest_df = pd.read_csv(f'{DATA_DIR}/Total_balanced_test_data.csv', encoding='CP949')\ntrain_audios = getAudios(train_df,1)\ntest_audios = getAudios(test_df,0)\ntrain_label = train_df['상황'].apply(lambda x: 1 if x == 'fear' else 0).values","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:38:20.374518Z","iopub.execute_input":"2024-11-17T03:38:20.375224Z","iopub.status.idle":"2024-11-17T03:39:07.332187Z","shell.execute_reply.started":"2024-11-17T03:38:20.375179Z","shell.execute_reply":"2024-11-17T03:39:07.331244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\nfor fold_idx, (train_indices, val_indices) in enumerate(skf.split(train_label, train_label)):\n    # 데이터 분리\n    print(train_indices)\n    print(len(train_audios))\n    train_fold_audios = [train_audios[train_index] for train_index in train_indices]\n    val_fold_audios = [train_audios[val_index] for val_index in val_indices]\n    \n    # 라벨 분리\n    train_fold_label = train_label[train_indices]\n    val_fold_label = train_label[val_indices]\n    \n    # 훈련 및 검증 데이터셋 생성\n    train_fold_ds = MyDataset(train_fold_audios, audio_feature_extractor, train_fold_label)\n    val_fold_ds = MyDataset(val_fold_audios, audio_feature_extractor, val_fold_label)\n    train_fold_dl = DataLoader(train_fold_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n    val_fold_dl = DataLoader(val_fold_ds, batch_size=BATCH_SIZE * 2, collate_fn=collate_fn)\n    \n    # 체크포인트 설정\n    checkpoint_acc_callback = ModelCheckpoint(\n        monitor='val_acc', # 검증 정확도 높은 모델 저장\n        dirpath=MODEL_DIR,\n        filename=f'fold_{fold_idx}_{{epoch:02d}}-{{val_acc:.4f}}-{{train_acc:.4f}}',\n        save_top_k=1,\n        mode='max'\n    )\n\n    # 모델 초기화\n    my_lit_model = MyLitModel(\n        audio_model_name=audio_model_name,\n        num_labels=1,               # 이진 분류를 위한 num_labels = 1\n        n_layers=1, projector=True, classifier=True, dropout=0.2, lr_decay=0.8, fold_idx=fold_idx\n    )\n\n    \n    # 학습률 모니터링 콜백\n    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n    \n    # 학습 실행 설정\n    trainer = pl.Trainer(\n        accelerator='cuda', \n        max_epochs=15,\n        precision=16, # 16-bit 연산\n        val_check_interval=0.50, # 에포크의 20%마다 검증 수행\n        callbacks=[checkpoint_acc_callback, lr_monitor], # 최고 성능 모델 저장\n    )\n\n    # 학습 실행\n    trainer.fit(my_lit_model, train_fold_dl, val_fold_dl)\n    \n\n\n    # 메모리 해제\n    del my_lit_model","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:27:08.681977Z","iopub.execute_input":"2024-11-14T02:27:08.682412Z","iopub.status.idle":"2024-11-14T02:27:31.721298Z","shell.execute_reply.started":"2024-11-14T02:27:08.682373Z","shell.execute_reply":"2024-11-14T02:27:31.714598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#테스트 라벨 \ntest_label = test_df['상황'].apply(lambda x: 1 if x == 'fear' else 0).values\n#test_label = test_df['label'].apply(lambda x: 1 if x == 1 else 0).values # 데이콘 전용 \n# 테스트 데이터셋 생성\ntest_ds = MyDataset(test_audios, audio_feature_extractor)\n# 데이터셋 로드해 배치 단위로 처리\ntest_dl = DataLoader(test_ds, batch_size=BATCH_SIZE*2, collate_fn=collate_fn)\n# 사전 학습된 모델 로드 및 예측 설정\n#MODEL_DIR = '/kaggle/input/1117model'\npretrained_models = list(map(lambda x: os.path.join(MODEL_DIR,x),os.listdir(MODEL_DIR)))\nprint(pretrained_models)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:40:44.366648Z","iopub.execute_input":"2024-11-17T03:40:44.367067Z","iopub.status.idle":"2024-11-17T03:40:44.379870Z","shell.execute_reply.started":"2024-11-17T03:40:44.367027Z","shell.execute_reply":"2024-11-17T03:40:44.378862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds = []\ntrainer = pl.Trainer(\n    accelerator='cuda', \n    precision=16,\n)\n\n# 각 모델에 대해 예측 수행\nfor pretrained_model_path in pretrained_models:\n    pretrained_model = MyLitModel.load_from_checkpoint(\n        pretrained_model_path,\n        audio_model_name=audio_model_name,\n        num_labels=NUM_LABELS,\n    )\n    test_pred = trainer.predict(pretrained_model, test_dl)\n    test_pred = torch.cat(test_pred).detach().cpu().numpy()\n    test_preds.append(test_pred)\n    del pretrained_model\n\n    # 모델의 예측값 평균 (모델 앙상블) 혹은 마지막 모델의 예측 사용\n    final_pred = np.mean(test_preds, axis=0)  # 예측값의 평균\n    final_pred = (final_pred > 0.5).astype(int)  # 0.5 기준으로 이진화\n    \n    accuracy = accuracy_score(test_label, final_pred)\n    print(pretrained_model_path)\n    print(f\"테스트 데이터 정확도: {accuracy * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-11-17T03:40:47.750144Z","iopub.execute_input":"2024-11-17T03:40:47.750537Z","iopub.status.idle":"2024-11-17T03:43:26.254718Z","shell.execute_reply.started":"2024-11-17T03:40:47.750499Z","shell.execute_reply":"2024-11-17T03:43:26.253567Z"},"trusted":true},"outputs":[],"execution_count":null}]}